I"Ì%<p>One of the primary applications of computer vision is object detection, which is used to locate and count objects in images or video frames. Object detection has thousands of practical applications and is one of the most commonly used forms of computer vision.</p>

<p>Typically, an object detector detects an object by drawing a rectangle (‚Äúbounding box‚Äù) around the object. This means that the training data also needs to be annotated with bounding boxes, which can be time consuming and tedious.</p>

<p>This begs the question: are bounding boxes always necessary? Are there more efficient ways to do object detection? In this article, we compare two platforms that use different object detection methods to make a case for an alternative to bounding boxes ‚Äî center points.</p>

<h2 id="when-are-bounding-boxes-unnecessary">When are bounding boxes unnecessary?</h2>

<p>If you want to know the rough size or extent of a detected object, you need bounding boxes. For example, if you‚Äôre detecting license plates so you can blur them, you need to know the extent of the plate.</p>

<p>But there are many cases when bounding boxes are unnecessary:</p>

<ul>
  <li>When you want to detect the count of objects in an image. For example, counting the number of homes visible in aerial photographs.</li>
  <li>When you want to determine the position of objects within the frame without needing to understand their boundaries or size. For example, detecting if your product is on the top, middle, or bottom shelf in a grocery store.</li>
  <li>When you want to detect the presence or absence of an object when the object you‚Äôre detecting is small in relation to the rest of the image. For example, <a href="https://www.nyckel.com/blog/gardyn-reduces-workload-by-70-while-growing-2x-after-implementing-computer-vision/">Gardyn detects the presence of specific fruits</a> in their indoor gardens. Some users would default to using <a href="https://www.nyckel.com/docs/image-classification-quickstart">image classification</a> for this use case. However, since the fruits are very small compared to the entire image, using object detection yields better results.</li>
</ul>

<p>From our conversations with Nyckel customers, the three bullet points above cover a significant chunk of object detection use cases. Any efficiencies gained from ditching bounding boxes in favor of center points would be compelling.</p>

<h2 id="bounding-boxes-vs-center-points">Bounding boxes vs. center points</h2>

<p>When an object detector uses bounding boxes, it draws rectangular boxes around each object the model detects. For training the model, annotators also need to draw boxes around the targeted objects. Annotating bounding boxes requires multiple clicks, or a click and drag, with extra attention paid to make sure the box doesn‚Äôt cover more or less than the bounds of the object. Annotation requires a lot of time and attention to detail.</p>

<p>Fortunately, boxing is not the only way to do object detection. Pointing to an object‚Äôs center (‚Äúcenter detection‚Äù) is another way and requires only specifying the position of a single point. We predict that annotating training data for center detection models is significantly faster. Let‚Äôs do a quick experiment to verify that claim.</p>

<h2 id="lets-experiment-annotating-bounding-boxes-vs-center-points">Let‚Äôs experiment: annotating bounding boxes vs. center points</h2>

<p>Our hypothesis is that annotating with points is faster than bounding boxes. We tested our hypothesis by comparing two platforms that differ in this labeling procedure:¬†<a href="https://cloud.google.com/vertex-ai/docs/image-data/object-detection/train-model">Vertex AI</a>, which uses bounding boxes, and¬†<a href="https://www.nyckel.com/docs/detection-quickstart">Nyckel</a>, which uses center points. We measured the time it took to correctly annotate the same 50 images on each platform.</p>

<p><em>Note: While we have a clear bias toward Nyckel‚Äôs product, we used a contractor who isn‚Äôt as familiar with our product as our internal team to conduct this experiment. To exclude one-time learning costs from the experiment, we asked him to familiarize himself with both products before conducting the experiment.</em></p>

<h3 id="the-input-data">The input data</h3>

<p><a href="https://roboflow.com">Roboflow</a> has a set of 100 benchmarking datasets, called <a href="https://universe.roboflow.com/roboflow-100">Roboflow 100</a>, available for this kind of experiment. We used their <a href="https://universe.roboflow.com/roboflow-100/grass-weeds">grass weeds data</a> and selected 50 images at random from the dataset. Each image contained anywhere from one to 24 weeds. These are the kind of images included in the dataset:</p>

<figure class="figure">

    

    

    

    
        <div class="post-img">
        <!-- Add the 'boxed' class if 'box' attribute is 'yes' -->
        <img src="../images/input-data-roboflow-grass.webp" alt="Roboflow 100 grass weeds data examples" class="  " style="border-radius: 20px;" srcset="../images/input-data-roboflow-grass.webp 2x%" loading="lazy" />
    </div>
    <figcaption>Sample images from the Roboflow 100 dataset</figcaption>
        

</figure>

<h3 id="our-experience-annotating-with-vertex-ais-bounding-boxes">Our experience annotating with Vertex AI‚Äôs bounding boxes</h3>

<p>When using Vertex AI, placing boxes near the edge of the frame sometimes resulted in misfires, adding a second or two each time. This was particularly a problem on the left margin because the UI only likes users to drag a rectangle from left to right. Also, correctly getting the minimal bounding box that includes a whole object doesn‚Äôt always work on the first try.</p>

<figure class="figure">

    

    

    

    
        <div class="post-img">
        <!-- Add the 'boxed' class if 'box' attribute is 'yes' -->
        <img src="../images/annotate-data-object-detection-bounding-boxes.gif" alt="Annotate data using object detection bounding boxes" class="  " style="border-radius: 20px;" srcset="../images/annotate-data-object-detection-bounding-boxes.gif 2x%" loading="lazy" />
    </div>
    <figcaption>User draws bounding boxes to annotate data for object detector</figcaption>
        

</figure>

<h3 id="our-experience-annotating-with-nyckels-center-points">Our experience annotating with Nyckel‚Äôs center points</h3>

<p>As you will see in the results below, annotating points was significantly faster. In addition, a noticeable difference with Nyckel is that Nyckel trains the model while you annotate, showing its predictions on training images after you‚Äôve annotated just a handful of them. These model predictions are startlingly accurate, and they guide the eye in annotating subsequent images.</p>

<p>However, the suggested annotations occasionally include false positives and sometimes fail to identify borderline object cases, meaning that Nyckel‚Äôs suggestions need to be verified by a human eye during the annotating process.</p>

<figure class="figure">

    

    

    

    
        <div class="post-img">
        <!-- Add the 'boxed' class if 'box' attribute is 'yes' -->
        <img src="../images/annotate-data-object-detection-center-pointing.gif" alt="Annotate data using object detection center pointing" class="  " style="border-radius: 20px;" srcset="../images/annotate-data-object-detection-center-pointing.gif 2x%" loading="lazy" />
    </div>
    <figcaption>User annotates data using center points for object detector</figcaption>
        

</figure>

<h3 id="the-results">The results</h3>

<figure class="figure">

    

    

    

    
        <div class="post-img">
        <!-- Add the 'boxed' class if 'box' attribute is 'yes' -->
        <img src="../images/Bounding-box-bar-graph-v2.webp" alt="Annotate data using object detection center pointing" class="  " style="border-radius: 20px;" srcset="../images/Bounding-box-bar-graph-v2.webp 2x%" loading="lazy" />
    </div>
    <figcaption></figcaption>
        

</figure>

<p>The difference is pretty striking between the two methods of object annotation: a factor of 3 difference! The additional clicks and image inspection time for the bounded box method quickly add up. If your use case doesn‚Äôt need bounding boxes, platforms using center-point detection have a clear advantage in annotation time and tedium.</p>

<h3 id="what-about-model-accuracy">What about model accuracy?</h3>

<p>So, how do the models generated by Nyckel and Vertex AI compare for this data? Great question! Stay tuned; we‚Äôll be answering that question soon through a benchmark of the performance of different object detection products. In the meantime, you can <a href="https://www.nyckel.com/docs/detection-quickstart">try us out</a> or read about how <a href="https://www.nyckel.com/blog/gardyn-reduces-workload-by-70-while-growing-2x-after-implementing-computer-vision/">Gardyn picked us</a> over Azure because of better model accuracy.</p>

<h2 id="conclusion-bounding-boxes-vs-center-points">Conclusion: Bounding boxes vs. center points</h2>

<p>Annotating center points is 3x less time-consuming than annotating bounding boxes. If you only need to detect the presence, number, and/or relative positions of objects, you are working too hard if you are using bounding boxes.</p>

<p>____</p>

<p>Thinking about deploying object detection? With Nyckel, you can annotate, train, and deploy an object detection model in minutes. Explore our <a href="https://www.nyckel.com/docs/detection-quickstart">object detection clickable demo</a> or <a href="https://nyckel.com/console">sign up for a free account</a> to try it out for yourself.</p>
:ET