I"d<p>In an earlier post of ours, <em><a href="https://www.nyckel.com/blog/four-steps-to-improving-your-content-moderation/">4 Steps to Improved Content Moderation</a></em>, we outline the stages of maturity most businesses go through when it comes to content moderation:</p>

<ol>
  <li>Realize you have a content moderation problem.</li>
  <li>Use simple heuristics like word blacklists. This works for a while until you realize that the blacklists are finding too many false positives and negatives, especially as users change their behavior to circumvent the list.</li>
  <li>Use a pre-trained machine learning content moderation model or API. This also works fine until you run into the same issue as with the blacklists: the false positives and false negatives are high, and the pre-trained model doesn’t cater to the unique communication patterns of your community.</li>
  <li>Finally, you use a machine learning model or API that is customized to your data, improves your false positives and negatives, and <a href="https://www.nyckel.com/blog/ai-content-moderation-best-practices/">continuously learns</a> from changing user behavior.</li>
</ol>

<p>At Nyckel, we are in the business of providing ML models customized to your data. Unsurprisingly, we claim that customized ML models are the ideal solution to your content moderation needs. In this post, we put that claim to the test by benchmarking a custom-trained Nyckel text classification function against a word blacklist and pre-trained content moderation APIs.</p>

<h2 id="the-moderators-we-tested">The moderators we tested</h2>

<h3 id="simple-blacklist-moderator">Simple blacklist moderator</h3>

<ul>
  <li><a href="https://rapidapi.com/community/api/purgomalum-1">PurgoMalum</a> is a free toxicity moderator based on a simple internal profanity list, available on RapidAPI, with unlimited use.</li>
</ul>

<h3 id="pre-trained-ml-moderators">Pre-trained ML moderators</h3>

<ul>
  <li><a href="https://rapidapi.com/jing-is-coding-jing-is-coding-default/api/detoxify">Detoxify</a> is a freemium ML-based toxicity and profanity moderator available on RapidAPI, limited to 10,000 requests per month.</li>
  <li><a href="https://perspectiveapi.com">Perspective API</a> is a free ML-based toxicity checker that runs on Google Cloud Console.</li>
</ul>

<h3 id="custom-ml-moderators">Custom ML moderators</h3>

<ul>
  <li><a href="https://www.nyckel.com">Nyckel</a> is an ML platform that can quickly train models based on your data without any ML expertise required. It provides various ML functions, like image classification, object detection, and semantic search, but we’ll use Nyckel’s text classification function for this post.</li>
</ul>

<h2 id="the-data">The data</h2>

<p>We have taken a public dataset from Kaggle, <em><a href="https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset">Hate Speech and Offensive Language Dataset</a></em>, which contains Tweets tagged as hate speech (0), offensive language (1), or neither (2). For benchmarking purposes, we can re-classify the text as simply toxic (1) or not toxic (0). </p>

<p>As the CSV file comes pre-sorted alphanumerically, first, we randomize it. We use the first 5000 data points in this randomized set to train Nyckel. Then we use the next 100 data points to test the accuracy of each content moderation API.</p>

<h2 id="the-benchmarking-process">The benchmarking process</h2>

<p>The <a href="https://github.com/NyckelAI/codesamples/blob/main/kaggle_toxicity_benchmark/kaggle_toxicity_benchmark.ipynb">code</a> and data for the benchmark are available on <a href="https://github.com/NyckelAI/codesamples/tree/main/kaggle_toxicity_benchmark">github</a>. We measure the accuracy of each moderator by feeding each of them the same set of 100 tweets. We then compare the result returned by the moderator against the toxicity labels provided in the dataset. The results are available in data files in the github repo. For example, Nyckel’s results are <a href="https://github.com/NyckelAI/codesamples/blob/main/kaggle_toxicity_benchmark/dataNyckel.csv">here</a>.</p>

<h3 id="training-nyckel">Training Nyckel</h3>

<p>Unlike the other content moderators, we need to train Nyckel since it’s a custom solution. To train Nyckel, we use the first 5000 tweets and toxicity labels from the dataset to train the model through the web app. When testing accuracy, we make sure that the data we use is not in the training data. This <a href="https://www.nyckel.com/docs/quickstart">text classification quickstart</a> has a clickable walkthrough for training a text classification function.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Accuracy</th>
      <th>False Positives</th>
      <th>False Negatives</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>PurgoMalum Blacklist</td>
      <td>88%</td>
      <td>11</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Detoxify</td>
      <td>84%</td>
      <td>11</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Perspective</td>
      <td>86%</td>
      <td>11</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Nyckel</td>
      <td>93%</td>
      <td>6</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>Nyckel outperforms the other toxicity moderators significantly. On further inspection into the false positive result identified by Nyckel (and others), it was actually a true positive, misidentified within the original CSV file. </p>

<p>Training Nyckel to identify toxic comments is simple, even without any ML expertise, and the results beat out competitive solutions. If you’re interested in learning more, read how Nyckel-powered content moderation helped <a href="https://www.nyckel.com/blog/how-whats-that-charge-quadrupled-ad-revenue-using-nyckel/">4x the revenue of one of our customers</a>, or <a href="https://www.nyckel.com/console">try us out</a> for free to see how it could work for your use case.</p>
:ET