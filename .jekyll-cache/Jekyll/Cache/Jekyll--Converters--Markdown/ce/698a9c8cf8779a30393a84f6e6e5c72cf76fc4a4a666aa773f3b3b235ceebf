I"⁄<p>Subreddit moderators don‚Äôt have an easy job. These are the hard-working people who lock down comments when they‚Äôre getting overly contentious, block NSFW content, and reject posts for straying from community guidelines, like a fence striking its way across the landscape in a picture posted in <a href="https://www.reddit.com/r/EarthPorn">r/EarthPorn</a>.</p>

<p><em>EarthPorn: The subreddit for gorgeous images of nature (that at least appear to be) untouched by the human hand.</em></p>

<p>Recently, NewScientist reported that ‚Äú<a href="https://www.newscientist.com/article/2325828-reddit-moderators-do-3-4-million-worth-of-unpaid-work-each-year/">Reddit moderators do $3.4 million worth of unpaid work each year</a>.‚Äù These volunteer mods are doing content moderation work for free, simply for the love of their passions.</p>

<p>But surely machine learning (ML) can tell a fence from a backdrop of nature? Couldn‚Äôt Reddit be deploying ML to do this type of content moderation instead? We decided to experiment with Nyckel to see how quickly we could train an image classification system to moderate r/EarthPorn images effectively.</p>

<p>[For text moderation as opposed to images, take a look at how Nyckel <a href="https://www.nyckel.com/blog/custom-spam-filter-gust-case-study/">helped Gust stop spam</a> and allowed <a href="https://www.nyckel.com/blog/content-moderation-of-social-network-taimi-case-study/">Taimi to auto-moderate their social network</a>.]</p>

<h2 id="our-starting-point-moderation-rules-for-rearthporn">Our starting point: Moderation rules for r/EarthPorn</h2>

<p>There are a few <a href="https://www.reddit.com/r/earthporn/about/rules">community guidelines for posting in r/EarthPorn</a>, but a core guideline that we can assess from a machine learning moderation point of view, is:</p>

<p><em>No images with human-made objects, people or animals.</em></p>

<p>For simplification for our demo, we are shortening this to ‚Äúno images with human-made objects.‚Äù More rules can always be added later.</p>

<p>Let‚Äôs test to validate our auto-moderation idea.</p>

<h2 id="step-1-crawl-the-rearthporn-subreddit-for-the-good-data">Step 1: Crawl the r/EarthPorn subreddit for the ‚ÄòGood‚Äô data</h2>

<p>To get the ‚ÄòGood‚Äô data, let‚Äôs retrieve the moderated content from r/EarthPorn. We‚Äôll want between 25 to 50 varied images.</p>

<p>Rather than building a custom scraper, we‚Äôre using <a href="https://github.com/ClarityCoders/RedditImageScraper">Clarity Coders RedditImageScraper on GitHub</a>, which has a <a href="https://www.youtube.com/watch?v=sEIv8UcR3Go">YouTube walkthrough</a> and uses Reddit‚Äôs Python API Wrapper. Instead of running the scraper with a Reddit web app as suggested, we changed it to a personal use script, using a throwaway account‚Äôs credentials to scrape the images, which are then delivered neatly to the /images folder.</p>

<p>Tip: Once you have a list of images, you might need to clean this data by deleting any images where you find man-made objects the mods may have missed.</p>

<h2 id="step-2-search-the-web-for-the-bad-data">Step 2: Search the web for the ‚ÄòBad‚Äô data</h2>

<p>We won‚Äôt be able to crawl EarthPorn for rejected images, but we still need to find some ‚ÄòBad‚Äô data to train the model. For instance, here is an <a href="https://www.reddit.com/r/EarthPorn/comments/x0td1g/mt_mikinley_denali_national_park_oc_4627x3085/">example</a> of an image that got blocked as it had a man-made structure (a bridge) in it.</p>

<p>We will need to find other nature images that contain some man-made structures (e.g., road, bridge, building, power line, tower), making sure the images are varied, both in the type of nature and the type of man-made structure. Again, this will require around 25-50 such images.</p>

<h2 id="step-3-train-a-nyckel-image-classification-function-to-tell-if-an-image-has-man-made-structures-or-not">Step 3: Train a Nyckel image classification function to tell if an image has man-made structures or not</h2>

<p>Using the images we found in Steps 1 and 2, we can train a Nyckel classification function by assigning labels to both the ‚ÄòGood‚Äô and ‚ÄòBad‚Äô images. Then, we simply invoke the function with a new image to see whether it is a picture of nature without a man-made structure in it.</p>

<p>Try it for yourself with this clickable demo:</p>

<figure class="figure" style="width: 100%">
    <div style="position: relative; padding-bottom: calc(67.56756756756756% + 41px); height: 0;">
        <iframe src="https://demo.arcade.software/YRrSnFWDf854WlbS9Jts?embed" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border-radius: 5px;"></iframe>
    </div>
    
    <figcaption>Train an image classifier to auto-moderate Reddit</figcaption>
    
</figure>

<p>Is the function accurate enough yet to accurately approve or reject any image we give it? Probably not. However, human moderators can help train the model with each image rejection and approval.</p>

<h2 id="how-it-could-work-in-the-real-world-embedding-image-classification-in-moderator-workflows">How it could work in the real world: Embedding image classification in moderator workflows</h2>

<p>Our example above shows how to classify the images from r/EarthPorn retrospectively. However, this process could be embedded in the subreddit moderation workflow with <a href="https://www.nyckel.com/blog/custom-ai-assisted-content-moderation-pipeline/">Custom AI-Assisted Content Moderation</a>.</p>

<figure class="figure">

    

    

    

    
    <a href="https://www.nyckel.com/custom-content-moderation-api">
        
        <div class="post-img">
        <!-- Add the 'boxed' class if 'box' attribute is 'yes' -->
        <img src="../images/2022/content-moderation-workflow.webp" alt="AI-assisted content moderation workflow" class="" style="border-radius: 20px; max-width: 100%" srcset="../images/2022/content-moderation-workflow.webp 2x%" loading="lazy" />
    </div>
    <figcaption>AI-assisted content moderation workflow</figcaption>
        
    </a>
    

</figure>

<p>Each time a moderator manually approves or disapproves a post, the image and its approval or disapproval tag is routed to Nyckel via the API, so AI-assisted moderation gets more accurate with each manual action.</p>

<p>By giving moderators a list sorted by confidence (lowest to highest), moderators can approve or disapprove the most difficult classifications first, in a priority system which will not only help save time but improve the AI model.</p>

<p>When the AI reaches a high degree of accuracy, after 100 or so photos, then it can be trusted to auto-approve or reject submissions that have the highest confidence.</p>

<h2 id="auto-moderation-is-essential-for-any-social-platform">Auto-moderation is essential for any social platform</h2>

<p>If you‚Äôre running a social platform, moderation is essential. While Reddit is using auto-moderation, its <a href="https://www.reddit.com/r/AutoModerator/wiki/index/">AutoModerator system</a> doesn‚Äôt do much ML, and there is no tailoring to the rules of a subreddit. Instead of using volunteers to do the majority of the work, it should be ML augmented by humans.</p>

<p>With simple auto-mod systems like we made with Nyckel that learn with each mod‚Äôs approval or disapproval, those unpaid volunteers will have a whole lot more time on their hands to enjoy the subreddit they‚Äôre so passionate about. Plus, <a href="https://www.nyckel.com/custom-content-moderation-api">Nyckel has a custom content moderation API</a> to boot.</p>

<p>Interested in giving Nyckel a spin yourself? <a href="https://www.nyckel.com/console">Sign up for a free account</a> at any time, and <a href="mailto:feedback@nyckel.com">reach out to us</a> with any questions.</p>
:ET